{"cells":[{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["from torchvision.transforms import transforms\n","from torch.utils.data import DataLoader\n","from torchvision.datasets import ImageFolder\n","from torch.utils.data import Dataset\n","from torchvision.io import read_image, ImageReadMode\n","import numpy\n","from torch.optim import Adam\n","from torch.autograd import Variable\n","import torchvision\n","from torch.utils.data import Dataset\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch\n","import glob\n","import os"]},{"cell_type":"code","execution_count":185,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["2527\n"]}],"source":["# Loading and normalizing the data.\n","# Define transformations for the training and test sets\n","\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","])\n","\n","train_dataset = ImageFolder(root='./../archive/image-data', transform=transform)\n","print(len(train_dataset))"]},{"cell_type":"code","execution_count":199,"metadata":{},"outputs":[],"source":["class CustomDataset(Dataset):\n","    def __init__(self, root_dir, transform):\n","        self.transform = transform\n","        self.image_paths = []\n","        for ext in ['png', 'jpg']:\n","            self.image_paths += glob.glob(os.path.join(root_dir, '*', f'*.{ext}'))\n","        class_set = set()\n","        for path in self.image_paths:\n","            class_set.add(os.path.dirname(path))\n","        self.class_lbl = { cls: i for i, cls in enumerate(sorted(list(class_set)))}\n","        print(self.class_lbl)\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        img = read_image(self.image_paths[idx], ImageReadMode.RGB).float()\n","        cls = os.path.basename(os.path.dirname(self.image_paths[idx]))\n","        print(f\"Extracted class: {cls}\")\n","        label = self.class_lbl[cls]\n","\n","        return self.transform(img), torch.tensor(label)\n"]},{"cell_type":"code","execution_count":200,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'./../archive/image-data/cardboard': 0, './../archive/image-data/glass': 1, './../archive/image-data/metal': 2, './../archive/image-data/paper': 3, './../archive/image-data/plastic': 4, './../archive/image-data/trash': 5}\n","2527\n"]}],"source":["dataset = CustomDataset('./../archive/image-data', transform)\n","print(len(dataset))\n","splits = [0.5, 0.25, 0.25]"]},{"cell_type":"code","execution_count":201,"metadata":{},"outputs":[],"source":["split_sizes = []\n","for sp in splits[:-1]:\n","    split_sizes.append(int(sp * len(dataset)))\n","split_sizes.append(len(dataset) - sum(split_sizes))\n"]},{"cell_type":"code","execution_count":202,"metadata":{},"outputs":[],"source":["train_set, test_set, val_set = torch.utils.data.random_split(dataset, split_sizes)"]},{"cell_type":"code","execution_count":203,"metadata":{},"outputs":[],"source":["dataloaders = {\n","    \"train\": DataLoader(train_set, batch_size=12, shuffle=True),\n","    \"test\": DataLoader(test_set, batch_size=12, shuffle=False),\n","    \"val\": DataLoader(val_set, batch_size=12, shuffle=False)\n","}\n","\n","for images, labels in dataloaders[\"\"]:"]},{"cell_type":"code","execution_count":204,"metadata":{},"outputs":[],"source":["class Network(torch.nn.Module):\n","    def __init__(self):\n","        super(Network, self).__init__()\n","\n","        self.linear1 = torch.nn.Linear(3072, 256)\n","        self.activation = torch.nn.ReLU()\n","        self.linear2 = torch.nn.Linear(256, 6)\n","\n","    def forward(self, x):\n","        x = self.linear1(x)\n","        x = self.activation(x)\n","        x = self.linear2(x)\n","        x = self.softmax(x)\n","        return x\n","    \n","model = Network()\n"]},{"cell_type":"code","execution_count":205,"metadata":{},"outputs":[],"source":["from torch.optim import Adam\n"," \n","    \n","# Define the loss function with Classification Cross-Entropy loss and an optimizer with Adam optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = Adam(model.parameters(), lr=0.001, weight_decay=0.0001)"]},{"cell_type":"code","execution_count":206,"metadata":{},"outputs":[],"source":["from torchvision.models import resnet50, ResNet50_Weights\n","model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n","model.fc = torch.nn.Sequential(\n","    torch.nn.Linear(2048, 256),\n","    torch.nn.ReLU(),\n","    torch.nn.Linear(256, 3)\n",")"]},{"cell_type":"code","execution_count":207,"metadata":{},"outputs":[],"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model.to(device)\n","\n","for param in model.parameters():\n","    param.requires_grad = False\n","\n","for param in model.fc.parameters():\n","    param.requires_grad = True"]},{"cell_type":"code","execution_count":208,"metadata":{},"outputs":[],"source":["metrics = {\n","    'train': {\n","         'loss': [], 'accuracy': []\n","    },\n","    'val': {\n","         'loss': [], 'accuracy': []\n","    },\n","}"]},{"cell_type":"code","execution_count":210,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0\n","-------- train --------\n","Extracted class: plastic\n"]},{"ename":"KeyError","evalue":"'plastic'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","Cell \u001b[0;32mIn[210], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m phase \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-------- \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mphase\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m --------\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m[\u001b[49m\u001b[43mphase\u001b[49m\u001b[43m]\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_grad_enabled\u001b[49m\u001b[43m(\u001b[49m\u001b[43mphase\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n","File \u001b[0;32m~/1/envs/pyaihack/lib/python3.12/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m~/1/envs/pyaihack/lib/python3.12/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m~/1/envs/pyaihack/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n","File \u001b[0;32m~/1/envs/pyaihack/lib/python3.12/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n","Cell \u001b[0;32mIn[199], line 20\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_paths[idx]))\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracted class: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_lbl\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(img), torch\u001b[38;5;241m.\u001b[39mtensor(label)\n","\u001b[0;31mKeyError\u001b[0m: 'plastic'"]}],"source":["for epoch in range(30):\n","    ep_metrics = {\n","        'train': {'loss': 0, 'accuracy': 0, 'count': 0},\n","        'val': {'loss': 0, 'accuracy': 0, 'count': 0},\n","    }\n","\n","    print(f'Epoch {epoch}')\n","\n","    for phase in ['train', 'val']:\n","        print(f'-------- {phase} --------')\n","        for images, labels in dataloaders[phase]:\n","            optimizer.zero_grad()\n","\n","            with torch.set_grad_enabled(phase == 'train'):\n","                output = model(images.to(device))\n","                \n","                # No need for one-hot encoding here\n","                loss = criterion(output, labels.to(device))\n","\n","                correct_preds = labels.to(device) == torch.argmax(output, dim=1)\n","                accuracy = correct_preds.sum().float() / len(labels)\n","\n","            if phase == 'train':\n","                loss.backward()\n","                optimizer.step()\n","\n","            ep_metrics[phase]['loss'] += loss.item()\n","            ep_metrics[phase]['accuracy'] += accuracy.item()\n","            ep_metrics[phase]['count'] += 1\n","\n","\n","        print(ep_metrics)\n","\n","        if ep_metrics[phase]['count'] > 0:  # Check to avoid division by zero\n","            ep_loss = ep_metrics[phase]['loss'] / ep_metrics[phase]['count']\n","            ep_accuracy = ep_metrics[phase]['accuracy'] / ep_metrics[phase]['count']\n","        else:\n","            ep_loss, ep_accuracy = float('nan'), float('nan')\n","\n","        print(f'Loss: {ep_loss}, Accuracy: {ep_accuracy}\\n')\n","\n","        metrics[phase]['loss'].append(ep_loss)\n","        metrics[phase]['accuracy'].append(ep_accuracy)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"pyaihack","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.6"}},"nbformat":4,"nbformat_minor":2}
